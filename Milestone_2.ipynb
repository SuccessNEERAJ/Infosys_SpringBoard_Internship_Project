{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install groq eventregistry transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc2FdwqBjarM",
        "outputId": "64e88748-1e86-4583-d471-470948ec2a8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting eventregistry\n",
            "  Downloading eventregistry-9.1.tar.gz (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from eventregistry) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from eventregistry) (1.17.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from eventregistry) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->eventregistry) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->eventregistry) (2.2.3)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: eventregistry\n",
            "  Building wheel for eventregistry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eventregistry: filename=eventregistry-9.1-py3-none-any.whl size=67624 sha256=963f77419f445b87a4d5e913b773673f954bc6ce4f013ddd27e955e57b04d811\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/b1/a3/96973dbeb71bb960bd053bfc7113194a3c35859407e20c907f\n",
            "Successfully built eventregistry\n",
            "Installing collected packages: eventregistry, groq\n",
            "Successfully installed eventregistry-9.1 groq-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z1-o9QBjRz7",
        "outputId": "37719bc3-bae4-4844-a726-490d264c9b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing article 1/20: Weeks after Ratan Tata's death, Tata Group boss N Chandrasekaran makes big announcement on jobs\n",
            "Sentiment: POSITIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 2/20: Scientists develop 'sustainable shield' tech on quest to harness limitless energy source: '[It] could play a vital role in the future'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 3/20: The Most Important Breakthroughs of 2024\n",
            "Sentiment: POSITIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 4/20: E-scooter ban reminder for rail passengers on the Airedale line\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 5/20: I Switched From An IPhone 12 Pro To An IPhone 16 Pro - Here Are 5 Things I Love And 3 Disappointments - Ny Breaking News\n",
            "Sentiment: POSITIVE (confidence: 1.00)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 6/20: Weeks after Ratan Tata's death, Tata Group boss N Chandrasekaran makes big announcement on jobs\n",
            "Sentiment: POSITIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 7/20: Little Rock's high-end home sales for December 2-6: Six of the most expensive properties | Northwest Arkansas Democrat-Gazette\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 8/20: Little Rock's high-end home sales for December 2-6: Six of the most expensive properties | Arkansas Democrat Gazette\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 9/20: COB (ASX) Broken Hill global cobalt province.\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 10/20: I switched from an iPhone 12 Pro to an iPhone 16 Pro - here are 5 things I love and 3 disappointments\n",
            "Sentiment: POSITIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 11/20: Fire breaks out in Tesla battery storage facility in Tilburg\n",
            "Sentiment: NEGATIVE (confidence: 0.95)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 12/20: Rust\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 13/20: Met Office 'be prepared' warning for Midlands over 'heavy snow and 75mph gusts'\n",
            "Sentiment: NEGATIVE (confidence: 0.98)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 14/20: Met Office forecasts snow on New Year's Day for Bolton\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 15/20: New Met Office snow warning as nine UK areas now set to be hit\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 16/20: New Met Office snow warning as nine UK areas now set to be hit\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 17/20: New Met Office snow warning as nine UK areas now set to be hit\n",
            "Sentiment: NEGATIVE (confidence: 0.99)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 18/20: Retiring fire chief reflects on 43-year career with Grand Blanc Township department\n",
            "Sentiment: NEGATIVE (confidence: 0.84)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 19/20: East Lancs told to brace for snow and strong winds as Met Office issues new warnings\n",
            "Sentiment: NEGATIVE (confidence: 1.00)\n",
            "Analysis saved to analysis_results.json\n",
            "\n",
            "Analyzing article 20/20: This Asus laptop is my go-to MacBook alternative - and it's on sale at Best Buy\n",
            "Sentiment: POSITIVE (confidence: 0.82)\n",
            "Analysis saved to analysis_results.json\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "from groq import Groq\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import json\n",
        "import os\n",
        "from eventregistry import *\n",
        "\n",
        "# API Configuration\n",
        "GROQ_API_KEY = \"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\"\n",
        "EVENT_REGISTRY_API_KEY = \"c3892498-706c-443a-a9a7-b194c52887b7\"\n",
        "\n",
        "# Model names\n",
        "SENTIMENT_MODEL = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# Initialize Groq client\n",
        "def initialize_groq():\n",
        "    return Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "def initialize_sentiment_analyzer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL)\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=SENTIMENT_MODEL,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    return sentiment_pipeline, tokenizer\n",
        "\n",
        "def truncate_for_model(text, tokenizer, max_length=512):\n",
        "    \"\"\"Truncate text to fit within model's token limit\"\"\"\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    if len(tokens) > max_length:\n",
        "        tokens = tokens[:max_length-1] + [tokenizer.sep_token_id]\n",
        "        text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "def truncate_for_llama(text, max_length=900):\n",
        "    \"\"\"Truncate text for LLaMA model\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) > max_length:\n",
        "        return ' '.join(words[:max_length]) + \"...\"\n",
        "    return text\n",
        "\n",
        "# Function to fetch news data from Event Registry\n",
        "def fetch_news():\n",
        "    try:\n",
        "        # Initialize EventRegistry\n",
        "        er = EventRegistry(apiKey=EVENT_REGISTRY_API_KEY)\n",
        "\n",
        "        # Create query for articles\n",
        "        q = QueryArticlesIter(\n",
        "            keywords = QueryItems.OR([\n",
        "       \"Lithium - Ion\", \"Batteries\", \"Electric Vehicles\"\n",
        "    \"Lithium shortage\",\n",
        "    \"Lithium\",\n",
        "    \"Cobalt\",\n",
        "       \"Mineral Mining\"\n",
        "            ]),\n",
        "            dateStart = (dt.now() - timedelta(days=7)).strftime('%Y-%m-%d'),\n",
        "            dateEnd = dt.now().strftime('%Y-%m-%d'),\n",
        "            dataType = [\"news\", \"blog\"],\n",
        "            lang = \"eng\"\n",
        "        )\n",
        "\n",
        "        # Execute query and collect results\n",
        "        articles = []\n",
        "        for article in q.execQuery(er, sortBy=\"date\", maxItems=20):\n",
        "            articles.append({\n",
        "                \"source\": {\n",
        "                    \"title\": article.get(\"source\", {}).get(\"title\", \"\")\n",
        "                },\n",
        "                \"title\": article.get(\"title\", \"\"),\n",
        "                \"body\": article.get(\"body\", \"\"),\n",
        "                \"dateTime\": article.get(\"dateTime\", \"\")\n",
        "            })\n",
        "\n",
        "        return {\"articles\": {\"results\": articles}}\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching news: {e}\")\n",
        "        return None\n",
        "\n",
        "# Risk analysis with Groq LLaMa\n",
        "def analyze_risk_with_llama(content, client):\n",
        "    try:\n",
        "        truncated_content = truncate_for_llama(content)\n",
        "\n",
        "        prompt = f\"\"\"Analyze the following news article for lithium-ion battery supply chain risks.\n",
        "\n",
        "        Consider these specific factors:\n",
        "        1. Raw Material Risks:\n",
        "           - Lithium, cobalt, nickel, and other critical material availability\n",
        "           - Price fluctuations and market dynamics\n",
        "           - Geopolitical tensions affecting material access\n",
        "\n",
        "        2. Manufacturing Risks:\n",
        "           - Production capacity issues\n",
        "           - Quality control challenges\n",
        "           - Technology changes or innovations\n",
        "\n",
        "        3. Geographic Risks:\n",
        "           - Regional concentration of suppliers\n",
        "           - Political instability in key regions\n",
        "           - Trade restrictions or policy changes\n",
        "\n",
        "        4. Industry Impact:\n",
        "           - Effects on EV and energy storage markets\n",
        "           - Impact on battery manufacturers\n",
        "           - Downstream effects on dependent industries\n",
        "\n",
        "        5. Mitigation Strategies:\n",
        "           - Alternative materials or technologies\n",
        "           - Supply diversification opportunities\n",
        "           - Strategic stockpiling considerations\n",
        "\n",
        "        Article: {truncated_content}\n",
        "\n",
        "        Provide a structured analysis of the identified risks and their potential impact on the lithium-ion battery supply chain.\"\"\"\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"llama3-70b-8192\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7,\n",
        "            max_tokens=1024,\n",
        "            top_p=1,\n",
        "            stream=False\n",
        "        )\n",
        "\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Groq LLaMa: {e}\")\n",
        "        return \"Error in risk analysis\"\n",
        "\n",
        "# Sentiment analysis with proper truncation\n",
        "def analyze_sentiment_with_model(content, sentiment_pipeline, tokenizer):\n",
        "    try:\n",
        "        # Properly truncate content for the model\n",
        "        truncated_content = truncate_for_model(content, tokenizer)\n",
        "\n",
        "        # Get sentiment prediction\n",
        "        result = sentiment_pipeline(truncated_content)[0]\n",
        "\n",
        "        # Format the result\n",
        "        return {\n",
        "            \"label\": result[\"label\"],\n",
        "            \"score\": float(result[\"score\"]),\n",
        "            \"analysis\": f\"Sentiment: {result['label']} (confidence: {result['score']:.2f})\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error with sentiment analysis: {e}\")\n",
        "        return {\n",
        "            \"label\": \"ERROR\",\n",
        "            \"score\": 0.0,\n",
        "            \"analysis\": \"Error in sentiment analysis\"\n",
        "        }\n",
        "\n",
        "# Aggregate data into structured format\n",
        "def aggregate_data(news_data):\n",
        "    try:\n",
        "        structured_data = []\n",
        "        for article in news_data.get('articles', {}).get('results', []):\n",
        "            structured_data.append({\n",
        "                \"source\": article.get('source', {}).get('title', ''),\n",
        "                \"title\": article.get('title', ''),\n",
        "                \"description\": article.get('body', ''),\n",
        "                \"content\": article.get('body', ''),\n",
        "                \"published_at\": article.get('dateTime', '')\n",
        "            })\n",
        "        return pd.DataFrame(structured_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error structuring data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main pipeline\n",
        "def main():\n",
        "    # Initialize models\n",
        "    groq_client = initialize_groq()\n",
        "    sentiment_pipeline, tokenizer = initialize_sentiment_analyzer()\n",
        "\n",
        "    # Fetch news data\n",
        "    news_data = fetch_news()\n",
        "    if not news_data:\n",
        "        return\n",
        "\n",
        "    # Aggregate data into structured format\n",
        "    structured_data = aggregate_data(news_data)\n",
        "    if structured_data is None or structured_data.empty:\n",
        "        print(\"No data to analyze\")\n",
        "        return\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = \"analysis_results\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Analyze risk and sentiment\n",
        "    results = []\n",
        "    for idx, row in structured_data.iterrows():\n",
        "        print(f\"\\nAnalyzing article {idx + 1}/{len(structured_data)}: {row['title']}\")\n",
        "\n",
        "        # Perform analyses\n",
        "        risk_analysis = analyze_risk_with_llama(row['content'], groq_client)\n",
        "        sentiment_analysis = analyze_sentiment_with_model(row['content'], sentiment_pipeline, tokenizer)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'title': row['title'],\n",
        "            'source': row['source'],\n",
        "            'published_at': row['published_at'],\n",
        "            'risk_analysis': risk_analysis,\n",
        "            'sentiment_analysis': sentiment_analysis\n",
        "        })\n",
        "\n",
        "        # Save interim results\n",
        "        with open(f\"{output_dir}/analysis_results.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Sentiment: {sentiment_analysis['label']} (confidence: {sentiment_analysis['score']:.2f})\")\n",
        "        print(\"Analysis saved to analysis_results.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}